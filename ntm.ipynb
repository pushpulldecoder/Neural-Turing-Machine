{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#     DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.N    = N\n",
    "        self.M    = M\n",
    "        self.size = [self.N, self.M]\n",
    "        \n",
    "        self.register_buffer(\"memory_bias\", torch.Tensor(N, M))\n",
    "        stdev = 1 / (np.sqrt(N + M))\n",
    "        nn.init.uniform_(self.memory_bias, -stdev, stdev)\n",
    "        \n",
    "    def reset(self, batch_size=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory     = self.memory_bias.clone().repeat(batch_size, 1, 1)\n",
    "        \n",
    "    def read(self, w):\n",
    "        data = torch.matmul(w.unsqueeze(1), self.memory).squeeze(1)\n",
    "        return data\n",
    "    \n",
    "    def write(self, w, e_gate, a_gate):\n",
    "        self.flashback = self.memory\n",
    "        self.memory    = torch.Tensor(self.batch_size, self.N, self.M)\n",
    "        erase          = torch.matmul(w.unsqueeze(-1), e_gate.unsqueeze(1))\n",
    "        add            = torch.matmul(w.unsqueeze(-1), a_gate.unsqueeze(1))\n",
    "        self.memory    = self.flashback * (1 - erase) + add\n",
    "        \n",
    "    def address(self, k, b, g, s, y, w_prev):\n",
    "        wc = self._similarity(k, b)\n",
    "        wg = self._interpolate(w_prev, wc, g)\n",
    "        w_ = self._shift(wg, s)\n",
    "        w  = self._sharpen(w_, y)\n",
    "        return w\n",
    "    \n",
    "    def _similarity(self, k, b):\n",
    "        k              = k.view(self.batch_size, 1, -1)\n",
    "        similarity     = F.cosine_similarity(self.memory + 1e-16, k + 1e-16, dim=-1)\n",
    "        content_weight = F.softmax(b * similarity, dim=1)\n",
    "        return content_weight\n",
    "    \n",
    "    def _interpolate(self, w_prev, wc, g):\n",
    "        focus = g * wc + (1 - g) * w_prev\n",
    "        return focus\n",
    "        \n",
    "    def _shift(self, wg, s):\n",
    "        shift = torch.zeros(wg.size())\n",
    "        for batch in range(self.batch_size):\n",
    "            shift[batch] = _convolve(wg[batch], s[batch])\n",
    "        return shift\n",
    "    \n",
    "    def _sharpen(self, w_, y):\n",
    "        w = w_ ** y\n",
    "        w = torch.div(w, torch.sum(w, dim=1).view(-1, 1) + 1e-16)\n",
    "        return w\n",
    "    \n",
    "    \n",
    "\n",
    "def _convolve(w, s):\n",
    "    t = torch.cat([w[-1:], w, w[:1]])\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadHead(nn.Module):\n",
    "    def __init__(self, memory, controller_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.memory             = memory\n",
    "        self.N, self.M          = self.memory.size\n",
    "        self.controller_size    = controller_size\n",
    "        \n",
    "        self.key                = nn.Linear(self.controller_size, self.M)\n",
    "        self.key_strength       = nn.Linear(self.controller_size, 1)\n",
    "        self.interpolation_gate = nn.Linear(self.controller_size, 1)\n",
    "        self.shift_weighting    = nn.Linear(self.controller_size, 3)\n",
    "        self.sharpen_factor     = nn.Linear(self.controller_size, 1)\n",
    "        \n",
    "        self.is_read_head       = True\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def _address(self, k, b, g, s, y, w_prev):\n",
    "        k = k.clone()\n",
    "        b = F.softplus(b)\n",
    "        g = torch.sigmoid(g)\n",
    "        s = torch.softmax(s, dim=1)\n",
    "        y = 1 + F.softplus(y)\n",
    "        \n",
    "        w = self.memory.address(k, b, g, s, y, w_prev)\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    def forward(self, controller_state, w_prev):\n",
    "        \n",
    "        k = self.key(controller_state)\n",
    "        b = self.key_strength(controller_state)\n",
    "        g = self.interpolation_gate(controller_state)\n",
    "        s = self.shift_weighting(controller_state)\n",
    "        y = self.sharpen_factor(controller_state)\n",
    "        \n",
    "        w    = self._address(k, b, g, s, y, w_prev)\n",
    "        \n",
    "        data = self.memory.read(w)\n",
    "        \n",
    "        return data, w\n",
    "    \n",
    "    def create_new_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.N)\n",
    "\n",
    "    def reset(self):\n",
    "        nn.init.xavier_uniform_(self.key.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.key_strength.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.interpolation_gate.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.shift_weighting.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.sharpen_factor.weight, gain=1.4)\n",
    "        \n",
    "        nn.init.normal_(self.key.bias, std=0.01)\n",
    "        nn.init.normal_(self.key_strength.bias, std=0.01)\n",
    "        nn.init.normal_(self.interpolation_gate.bias, std=0.01)\n",
    "        nn.init.normal_(self.shift_weighting.bias, std=0.01)\n",
    "        nn.init.normal_(self.sharpen_factor.bias, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteHead(nn.Module):\n",
    "    def __init__(self, memory, controller_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.memory             = memory\n",
    "        self.N, self.M          = self.memory.size\n",
    "        self.controller_size    = controller_size\n",
    "        \n",
    "        self.key                = nn.Linear(self.controller_size, self.M)\n",
    "        self.key_strength       = nn.Linear(self.controller_size, 1)\n",
    "        self.interpolation_gate = nn.Linear(self.controller_size, 1)\n",
    "        self.shift_weighting    = nn.Linear(self.controller_size, 3)\n",
    "        self.sharpen_factor     = nn.Linear(self.controller_size, 1)\n",
    "        self.erase              = nn.Linear(self.controller_size, self.M)\n",
    "        self.add                = nn.Linear(self.controller_size, self.M)\n",
    "        \n",
    "        self.is_read_head       = False\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def _address(self, k, b, g, s, y, w_prev):\n",
    "        k = k.clone()\n",
    "        b = F.softplus(b)\n",
    "        g = torch.sigmoid(g)\n",
    "        s = torch.softmax(s, dim=1)\n",
    "        y = 1 + F.softplus(y)\n",
    "        \n",
    "        w = self.memory.address(k, b, g, s, y, w_prev)\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    def forward(self, controller_state, w_prev):\n",
    "        \n",
    "        k = self.key(controller_state)\n",
    "        b = self.key_strength(controller_state)\n",
    "        g = self.interpolation_gate(controller_state)\n",
    "        s = self.shift_weighting(controller_state)\n",
    "        y = self.sharpen_factor(controller_state)\n",
    "        e = self.erase(controller_state)\n",
    "        a = self.add(controller_state)\n",
    "        \n",
    "        e = torch.sigmoid(e)\n",
    "        \n",
    "        w    = self._address(k, b, g, s, y, w_prev)\n",
    "        \n",
    "        self.memory.write(w, e, a)\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    def create_new_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.N)\n",
    "\n",
    "    def reset(self):\n",
    "        nn.init.xavier_uniform_(self.key.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.key_strength.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.interpolation_gate.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.shift_weighting.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.sharpen_factor.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.erase.weight, gain=1.4)\n",
    "        nn.init.xavier_uniform_(self.add.weight, gain=1.4)\n",
    "        \n",
    "        nn.init.normal_(self.key.bias, std=0.01)\n",
    "        nn.init.normal_(self.key_strength.bias, std=0.01)\n",
    "        nn.init.normal_(self.interpolation_gate.bias, std=0.01)\n",
    "        nn.init.normal_(self.shift_weighting.bias, std=0.01)\n",
    "        nn.init.normal_(self.sharpen_factor.bias, std=0.01)\n",
    "        nn.init.normal_(self.erase.bias, std=0.01)\n",
    "        nn.init.normal_(self.add.bias, std=0.01)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    def __init__(self, no_input, no_output, no_layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.no_input  = no_input\n",
    "        self.no_output = no_output\n",
    "        self.no_layer  = no_layer\n",
    "        self.size      = [self.no_input, self.no_output]\n",
    "        \n",
    "        self.lstm      = nn.LSTM(input_size =self.no_input,\n",
    "                                 hidden_size=self.no_output,\n",
    "                                 num_layers = self.no_layer)\n",
    "        \n",
    "        self.h_bias    = nn.Parameter(torch.randn(self.no_layer, 1, self.no_output) * 0.05)\n",
    "        self.c_bias    = nn.Parameter(torch.randn(self.no_layer, 1, self.no_output) * 0.05)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def forward(self, data, prev_state):\n",
    "        data          = data.unsqueeze(0)\n",
    "        output, state = self.lstm(data, prev_state)\n",
    "        return output.squeeze(0), state\n",
    "        \n",
    "    def create_new_state(self, batch_size):\n",
    "        h = self.h_bias.clone().repeat(1, batch_size, 1)\n",
    "        c = self.c_bias.clone().repeat(1, batch_size, 1)\n",
    "        \n",
    "        return h, c\n",
    "    \n",
    "    def reset(self):\n",
    "        for param in self.lstm.parameters():\n",
    "            if param.dim()==1:\n",
    "                nn.init.constant_(param, 0)\n",
    "            else:\n",
    "                stdev = 1 / (np.sqrt(self.no_input + self.no_output))\n",
    "                nn.init.uniform_(param, -stdev, stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, no_input, no_output, controller_size, controller_layer, no_head, N, M):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.no_input         = no_input\n",
    "        self.no_output        = no_output\n",
    "        self.controller_size  = controller_size\n",
    "        self.controller_layer = controller_layer\n",
    "        self.no_head          = no_head\n",
    "        \n",
    "        self.N                = N\n",
    "        self.M                = M        \n",
    "\n",
    "        self.memory           = Memory(self.N, self.M)\n",
    "        self.controller       = Controller(self.no_input + (self.M * self.no_head), self.controller_size, self.controller_layer)\n",
    "        self.head             = nn.ModuleList([])\n",
    "        \n",
    "        _, self.controller_size  = self.controller.size\n",
    "        \n",
    "        for head_no in range(self.no_head):\n",
    "            self.head += [\n",
    "                            ReadHead(self.memory, self.controller_size),\n",
    "                            WriteHead(self.memory, self.controller_size)\n",
    "            ]\n",
    "        self.no_read_head = 0\n",
    "        self.read         = []\n",
    "        for head in self.head:\n",
    "            if head.is_read_head:\n",
    "                read_bias = torch.randn(1, self.M) * 0.01\n",
    "                self.register_buffer(\"read{}_bias\".format(self.no_read_head), read_bias.data)\n",
    "                self.read += [read_bias]\n",
    "                self.no_read_head += 1\n",
    "        \n",
    "        self.fc = nn.Linear(self.controller_size + self.no_read_head * self.M, self.no_output)\n",
    "        self.reset()\n",
    "        \n",
    "    def create_new_state(self, batch_size):\n",
    "        read             = [r.clone().repeat(batch_size, 1) for r in self.read]\n",
    "        controller_state = self.controller.create_new_state(batch_size)\n",
    "        head_state       = [head.create_new_state(batch_size) for head in self.head]\n",
    "        return read, controller_state, head_state\n",
    "        \n",
    "    def init_sequence(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory.reset(batch_size)\n",
    "        self.previous_state = self.create_new_state(batch_size)\n",
    "        \n",
    "    def forward(self, x=None):\n",
    "        if x is None:\n",
    "            x = torch.zeros(self.batch_size, self.no_input)\n",
    "            \n",
    "        prev_read, prev_controller_state, prev_head_state = self.previous_state\n",
    "        \n",
    "        inp                                 = torch.cat([x] + prev_read, dim=1)\n",
    "        controller_output, controller_state = self.controller(inp, prev_controller_state)\n",
    "        \n",
    "        reads = []\n",
    "        head_state = []\n",
    "        for head, prev_head_state in zip(self.head, prev_head_state):\n",
    "            if head.is_read_head:\n",
    "                r, h_state = head(controller_output, prev_head_state)\n",
    "                reads += [r]\n",
    "            else:\n",
    "                h_state = head(controller_output, prev_head_state)\n",
    "            head_state += [h_state]\n",
    "            \n",
    "        out = torch.cat([controller_output] + reads, dim=1)\n",
    "        out = torch.sigmoid(self.fc(out))\n",
    "        \n",
    "        self.previous_state = (reads, controller_state, head_state)\n",
    "        \n",
    "        return out, self.previous_state\n",
    "        \n",
    "    def reset(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n",
    "        nn.init.normal_(self.fc.bias, std=0.01)\n",
    "        \n",
    "    def no_param(self):\n",
    "        no_param = 0\n",
    "        for param in self.parameters():\n",
    "            no_param += param.data.view(-1).size(0)\n",
    "        return no_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(no_batch, batch_size, seq_width, min_len, max_len):\n",
    "    \n",
    "    for batch_no in range(no_batch):\n",
    "        seq_len = np.random.randint(min_len, max_len)\n",
    "        seq     = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n",
    "        seq     = torch.from_numpy(seq)\n",
    "        \n",
    "        inp     = torch.zeros(seq_len+1, batch_size, seq_width+1)\n",
    "        inp[:seq_len, :, :seq_width] = seq\n",
    "        inp[seq_len, :, seq_width]   = 1\n",
    "        out     = seq.clone()\n",
    "        \n",
    "        yield batch_no+1, inp.float(), out.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_input         = 9\n",
    "no_output        = 8\n",
    "controller_size  = 100\n",
    "controller_layer = 1\n",
    "no_head          = 1\n",
    "N                = 128\n",
    "M                = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPIER = NTM(no_input=no_input, no_output=no_output, controller_size=controller_size, controller_layer=controller_layer, no_head=no_head, N=N, M=M).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_clean():\n",
    "    \"\"\"Clean the progress bar.\"\"\"\n",
    "    print(\"\\r{}\".format(\" \" * 80), end='\\r')\n",
    "\n",
    "\n",
    "def progress_bar(batch_num, report_interval, last_loss):\n",
    "    \"\"\"Prints the progress until the next report.\"\"\"\n",
    "    progress = (((batch_num-1) % report_interval) + 1) / report_interval\n",
    "    fill = int(progress * 40)\n",
    "    print(\"\\r[{}{}]: {} (Loss: {:.4f})\".format(\n",
    "        \"=\" * fill, \" \" * (40 - fill), batch_num, last_loss), end='')\n",
    "\n",
    "def save_checkpoint(net, name, args, batch_num, losses, costs, seq_lengths):\n",
    "    progress_clean()\n",
    "\n",
    "    basename = \"{}/{}-{}-batch-{}\".format(args.checkpoint_path, name, args.seed, batch_num)\n",
    "    model_fname = basename + \".model\"\n",
    "    LOGGER.info(\"Saving model checkpoint to: '%s'\", model_fname)\n",
    "    torch.save(net.state_dict(), model_fname)\n",
    "\n",
    "    # Save the training history\n",
    "    train_fname = basename + \".json\"\n",
    "    LOGGER.info(\"Saving model training history to '%s'\", train_fname)\n",
    "    content = {\n",
    "        \"loss\": losses,\n",
    "        \"cost\": costs,\n",
    "        \"seq_lengths\": seq_lengths\n",
    "    }\n",
    "    open(train_fname, 'wt').write(json.dumps(content))\n",
    "\n",
    "\n",
    "def clip_grads(net):\n",
    "    \"\"\"Gradient clipping to the range [10, 10].\"\"\"\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "def get_ms():\n",
    "    \"\"\"Returns the current time in miliseconds.\"\"\"\n",
    "    return time.time() * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_batch   = 50000\n",
    "batch_size = 1\n",
    "loss       = nn.BCELoss()\n",
    "optimizer  = torch.optim.RMSprop(COPIER.parameters(), momentum=0.9, alpha=0.95, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors     = []\n",
    "costs      = []\n",
    "seq_length = []\n",
    "start_ms = get_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Time: 88 ms                                                                \n",
      "Mean Cost: 39.815\n",
      "Mean Loss: 0.6929117912054061\n",
      "=====================================\n",
      "Mean Time: 88 ms                                                                \n",
      "Mean Cost: 37.475\n",
      "Mean Loss: 0.6854926851391793\n",
      "=====================================\n",
      "Mean Time: 82 ms                                                                \n",
      "Mean Cost: 32.75\n",
      "Mean Loss: 0.6770105096697807\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 34.53\n",
      "Mean Loss: 0.6714207538962365\n",
      "=====================================\n",
      "Mean Time: 85 ms                                                                \n",
      "Mean Cost: 33.145\n",
      "Mean Loss: 0.653018511235714\n",
      "=====================================\n",
      "Mean Time: 82 ms                                                                \n",
      "Mean Cost: 31.55\n",
      "Mean Loss: 0.6371622154116631\n",
      "=====================================\n",
      "Mean Time: 88 ms                                                                \n",
      "Mean Cost: 34.945\n",
      "Mean Loss: 0.6375371615588665\n",
      "=====================================\n",
      "Mean Time: 87 ms                                                                \n",
      "Mean Cost: 34.515\n",
      "Mean Loss: 0.6336222882568836\n",
      "=====================================\n",
      "Mean Time: 90 ms                                                                \n",
      "Mean Cost: 35.15\n",
      "Mean Loss: 0.6368603789806366\n",
      "=====================================\n",
      "Mean Time: 89 ms                                                                \n",
      "Mean Cost: 34.86\n",
      "Mean Loss: 0.6321978785842657\n",
      "=====================================\n",
      "Mean Time: 87 ms                                                                \n",
      "Mean Cost: 32.735\n",
      "Mean Loss: 0.6211468613892793\n",
      "=====================================\n",
      "Mean Time: 91 ms                                                                \n",
      "Mean Cost: 35.505\n",
      "Mean Loss: 0.6168952092528344\n",
      "=====================================\n",
      "Mean Time: 87 ms                                                                \n",
      "Mean Cost: 32.995\n",
      "Mean Loss: 0.6169592410326004\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 32.335\n",
      "Mean Loss: 0.6061160646378994\n",
      "=====================================\n",
      "Mean Time: 82 ms                                                                \n",
      "Mean Cost: 30.86\n",
      "Mean Loss: 0.5899750307202339\n",
      "=====================================\n",
      "Mean Time: 92 ms                                                                \n",
      "Mean Cost: 34.865\n",
      "Mean Loss: 0.6095391876250505\n",
      "=====================================\n",
      "Mean Time: 90 ms                                                                \n",
      "Mean Cost: 34.61\n",
      "Mean Loss: 0.6133967481739819\n",
      "=====================================\n",
      "Mean Time: 87 ms                                                                \n",
      "Mean Cost: 32.605\n",
      "Mean Loss: 0.586367378141731\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 31.725\n",
      "Mean Loss: 0.5962087765336037\n",
      "=====================================\n",
      "Mean Time: 84 ms                                                                \n",
      "Mean Cost: 31.415\n",
      "Mean Loss: 0.6017162602581084\n",
      "=====================================\n",
      "Mean Time: 89 ms                                                                \n",
      "Mean Cost: 33.22\n",
      "Mean Loss: 0.5933102795109153\n",
      "=====================================\n",
      "Mean Time: 92 ms                                                                \n",
      "Mean Cost: 33.895\n",
      "Mean Loss: 0.5939427346922457\n",
      "=====================================\n",
      "Mean Time: 87 ms                                                                \n",
      "Mean Cost: 31.565\n",
      "Mean Loss: 0.5894265495613218\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 31.505\n",
      "Mean Loss: 0.5880874391645193\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 30.745\n",
      "Mean Loss: 0.5715779672190547\n",
      "=====================================\n",
      "Mean Time: 79 ms                                                                \n",
      "Mean Cost: 28.08\n",
      "Mean Loss: 0.5612848053406924\n",
      "=====================================\n",
      "Mean Time: 89 ms                                                                \n",
      "Mean Cost: 32.61\n",
      "Mean Loss: 0.5712059437297284\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 31.53\n",
      "Mean Loss: 0.5634446961991489\n",
      "=====================================\n",
      "Mean Time: 90 ms                                                                \n",
      "Mean Cost: 32.525\n",
      "Mean Loss: 0.5569419605843723\n",
      "=====================================\n",
      "Mean Time: 85 ms                                                                \n",
      "Mean Cost: 30.19\n",
      "Mean Loss: 0.5629311700584367\n",
      "=====================================\n",
      "Mean Time: 88 ms                                                                \n",
      "Mean Cost: 30.145\n",
      "Mean Loss: 0.5502075304696337\n",
      "=====================================\n",
      "Mean Time: 85 ms                                                                \n",
      "Mean Cost: 29.015\n",
      "Mean Loss: 0.5462183670327068\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 29.435\n",
      "Mean Loss: 0.5435756279202179\n",
      "=====================================\n",
      "Mean Time: 90 ms                                                                \n",
      "Mean Cost: 30.985\n",
      "Mean Loss: 0.5460169444233179\n",
      "=====================================\n",
      "Mean Time: 85 ms                                                                \n",
      "Mean Cost: 29.005\n",
      "Mean Loss: 0.5383810158411506\n",
      "=====================================\n",
      "Mean Time: 90 ms                                                                \n",
      "Mean Cost: 30.1\n",
      "Mean Loss: 0.5351050817058421\n",
      "=====================================\n",
      "Mean Time: 86 ms                                                                \n",
      "Mean Cost: 28.405\n",
      "Mean Loss: 0.521223556085024\n",
      "=====================================\n",
      "Mean Time: 88 ms                                                                \n",
      "Mean Cost: 29.22\n",
      "Mean Loss: 0.5323615406779573\n",
      "=====================================\n",
      "Mean Time: 89 ms                                                                \n",
      "Mean Cost: 28.99\n",
      "Mean Loss: 0.5182041353569367\n",
      "=====================================\n",
      "Mean Time: 96 ms                                                                \n",
      "Mean Cost: 29.295\n",
      "Mean Loss: 0.5244816327467561\n",
      "=====================================\n",
      "Mean Time: 89 ms                                                                \n",
      "Mean Cost: 24.725\n",
      "Mean Loss: 0.4792963534308365\n",
      "=====================================\n",
      "Mean Time: 95 ms                                                                \n",
      "Mean Cost: 12.705\n",
      "Mean Loss: 0.35316998495836743\n",
      "=====================================\n",
      "Mean Time: 94 ms                                                                \n",
      "Mean Cost: 1.815\n",
      "Mean Loss: 0.1602942551660817\n",
      "=====================================\n",
      "Mean Time: 89 ms                                                                \n",
      "Mean Cost: 1.58\n",
      "Mean Loss: 0.09224163986044004\n",
      "=====================================\n",
      "Mean Time: 87 ms                                                                \n",
      "Mean Cost: 0.02\n",
      "Mean Loss: 0.019065622113121208\n",
      "=====================================\n",
      "Mean Time: 91 ms                                                                \n",
      "Mean Cost: 0.0\n",
      "Mean Loss: 0.0022380232104842433\n",
      "=====================================\n",
      "Mean Time: 89 ms                                                                \n",
      "Mean Cost: 0.38\n",
      "Mean Loss: 0.007804916020841119\n",
      "=====================================\n",
      "Mean Time: 88 ms                                                                \n",
      "Mean Cost: 0.0\n",
      "Mean Loss: 6.885103782337865e-05\n",
      "=====================================\n",
      "Mean Time: 90 ms                                                                \n",
      "Mean Cost: 0.0\n",
      "Mean Loss: 1.0642915518701558e-05\n",
      "=====================================\n",
      "Mean Time: 93 ms                                                                \n",
      "Mean Cost: 0.0\n",
      "Mean Loss: 1.619482492003499e-06\n",
      "=====================================\n",
      "Mean Time: 93 ms                                                                \n",
      "Mean Cost: 0.0\n",
      "Mean Loss: 2.3528569656150467e-07\n",
      "=====================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Network has successfully learnt to copy memory elements"
     ]
    }
   ],
   "source": [
    "for batch_no, x, y in dataloader(no_batch=no_batch, batch_size=batch_size, seq_width=8, min_len=1, max_len=20):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    inp_seq_len = x.size(0)\n",
    "    out_seq_len = y.size(0)\n",
    "    \n",
    "    LOGGER.info(\"Training model for %d batches (batch_size=%d)...\",\n",
    "                no_batch, batch_size)\n",
    "    \n",
    "    \n",
    "    COPIER.init_sequence(batch_size)\n",
    "    \n",
    "    for i in range(inp_seq_len):\n",
    "        COPIER(x[i])\n",
    "        \n",
    "    y_ = torch.zeros(y.size())\n",
    "    \n",
    "    for i in range(out_seq_len):\n",
    "        y_[i], _ = COPIER()\n",
    "        \n",
    "    error = loss(y_, y)\n",
    "    error.backward()\n",
    "    clip_grads(COPIER)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    y_binarized = y_.clone().data\n",
    "    y_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    cost = torch.sum(torch.abs(y_binarized - y.data))\n",
    "    \n",
    "    \n",
    "    errors.append(error.item())\n",
    "    costs.append(cost.item()/batch_size)\n",
    "    \n",
    "    seq_length += [y.size(0)]\n",
    "    \n",
    "    \n",
    "    progress_bar(batch_no, 200, error)\n",
    "\n",
    "        # Report\n",
    "    if batch_no % 200 == 0:\n",
    "        mean_loss = np.array(errors[-200:]).mean()\n",
    "        mean_cost = np.array(costs[-200:]).mean()\n",
    "        mean_time = int(((get_ms() - start_ms) / 200) / batch_size)\n",
    "        progress_clean()\n",
    "        print(\"Mean Time: {} ms\".format(mean_time))\n",
    "        print(\"Mean Cost: {}\".format(mean_cost))\n",
    "        print(\"Mean Loss: {}\".format(mean_loss))\n",
    "        print(\"=====================================\")\n",
    "        LOGGER.info(\"Batch %d Loss: %.6f Cost: %.2f Time: %d ms/sequence\",\n",
    "                    batch_no, mean_loss, mean_cost, mean_time)\n",
    "        start_ms = get_ms()\n",
    "\n",
    "#         # Checkpoint\n",
    "#     if (1000 != 0) and (batch_no % 1000 == 0):\n",
    "#         save_checkpoint(copier, \"copier\"+str(batch_no), args,\n",
    "#                         batch_0, losses, costs, seq_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network has successfully learnt to copy memory elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
